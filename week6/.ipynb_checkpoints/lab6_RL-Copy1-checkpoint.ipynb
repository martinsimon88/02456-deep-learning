{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Quadro K1000M (CNMeM is enabled with initial size: 45.0% of memory, cuDNN 5110)\n",
      "/home/simonx/.local/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "from lasagne.layers import InputLayer, DenseLayer\n",
    "from lasagne.nonlinearities import tanh, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    \"\"\"\n",
    "    Reinforcement Learning Agent\n",
    "    \n",
    "    This agent can learn to solve reinforcement learning tasks from\n",
    "    OpenAI Gym by applying the policy gradient method.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        # symbolic variables for state, action, and advantage\n",
    "        sym_state = T.fmatrix()\n",
    "        sym_action = T.ivector()\n",
    "        sym_advantage = T.fvector()\n",
    "        # policy network\n",
    "        l_in = InputLayer(shape=(None, n_inputs))\n",
    "        l_hid = DenseLayer(incoming=l_in, num_units=20, nonlinearity=tanh, name='hiddenlayer')\n",
    "        l_out = DenseLayer(incoming=l_hid, num_units=n_outputs, nonlinearity=softmax, name='outputlayer')\n",
    "        # get network output\n",
    "        eval_out = lasagne.layers.get_output(l_out, {l_in: sym_state}, deterministic=True)\n",
    "        # get trainable parameters in the network.\n",
    "        params = lasagne.layers.get_all_params(l_out, trainable=True)\n",
    "        # get total number of timesteps\n",
    "        t_total = sym_state.shape[0]\n",
    "        # loss function that we'll differentiate to get the policy gradient\n",
    "        loss = -T.log(eval_out[T.arange(t_total), sym_action]).dot(sym_advantage) / t_total\n",
    "        # learning_rate\n",
    "        learning_rate = T.fscalar()\n",
    "        # get gradients\n",
    "        grads = T.grad(loss, params)\n",
    "        # update function\n",
    "        updates = lasagne.updates.sgd(grads, params, learning_rate=learning_rate)\n",
    "        # declare training and evaluation functions\n",
    "        self.f_train = theano.function([sym_state, sym_action, sym_advantage, learning_rate], loss, updates=updates, allow_input_downcast=True)\n",
    "        self.f_eval = theano.function([sym_state], eval_out, allow_input_downcast=True)\n",
    "    \n",
    "    def learn(self, env, n_epochs=100, t_per_batch=10000, traj_t_limit=None,\n",
    "              learning_rate=0.1, discount_factor=1.0, n_early_stop=0):\n",
    "        \"\"\"\n",
    "        Learn the given environment by the policy gradient method.\n",
    "        \"\"\"\n",
    "        self.mean_train_rs = []\n",
    "        self.mean_val_rs = []\n",
    "        self.loss = []\n",
    "        for epoch in xrange(n_epochs):\n",
    "            # 1. collect trajectories until we have at least t_per_batch total timesteps\n",
    "            trajs = []; t_total = 0\n",
    "            while t_total < t_per_batch:\n",
    "                traj = self.get_trajectory(env, traj_t_limit, deterministic=False)\n",
    "                trajs.append(traj)\n",
    "                t_total += len(traj[\"r\"])\n",
    "            all_s = np.concatenate([traj[\"s\"] for traj in trajs])\n",
    "            # 2. compute cumulative discounted rewards (returns)\n",
    "            rets = [self._cumulative_discount(traj[\"r\"], discount_factor) for traj in trajs]\n",
    "            maxlen = max(len(ret) for ret in rets)\n",
    "            padded_rets = [np.concatenate([ret, np.zeros(maxlen-len(ret))]) for ret in rets]\n",
    "            # 3. compute time-dependent baseline\n",
    "            baseline = np.mean(padded_rets, axis=0)\n",
    "            # 4. compute advantages\n",
    "            advs = [ret - baseline[:len(ret)] for ret in rets]\n",
    "            all_a = np.concatenate([traj[\"a\"] for traj in trajs])\n",
    "            all_adv = np.concatenate(advs)\n",
    "            # 5. do policy gradient update step\n",
    "            loss = self.f_train(all_s, all_a, all_adv, learning_rate)\n",
    "            train_rs = np.array([traj[\"r\"].sum() for traj in trajs]) # trajectory total rewards\n",
    "            eplens = np.array([len(traj[\"r\"]) for traj in trajs]) # trajectory lengths\n",
    "            # compute validation reward\n",
    "            val_rs = np.array([self.get_trajectory(env, traj_t_limit, deterministic=True)['r'].sum() for _ in range(10)])\n",
    "            # update stats\n",
    "            self.mean_train_rs.append(train_rs.mean())\n",
    "            self.mean_val_rs.append(val_rs.mean())\n",
    "            self.loss.append(loss)\n",
    "            # print stats\n",
    "            print '%3d mean_train_r: %6.2f mean_val_r: %6.2f loss: %f' % (epoch+1, train_rs.mean(), val_rs.mean(), loss)\n",
    "            # render solution\n",
    "            #self.get_trajectory(env, traj_t_limit, render=True)\n",
    "            # check for early stopping: true if the validation reward has not changed in n_early_stop epochs\n",
    "            if n_early_stop and len(self.mean_val_rs) >= n_early_stop and \\\n",
    "                all([x == self.mean_val_rs[-1] for x in self.mean_val_rs[-n_early_stop:-1]]):\n",
    "                break\n",
    "    \n",
    "    def get_trajectory(self, env, t_limit=None, render=False, deterministic=True):\n",
    "        \"\"\"\n",
    "        Compute trajectroy by iteratively evaluating the agent policy on the environment.\n",
    "        \"\"\"\n",
    "        t_limit = t_limit or env.spec.timestep_limit\n",
    "        s = env.reset()\n",
    "        traj = {'s': [], 'a': [], 'r': [],}\n",
    "        for _ in xrange(t_limit):\n",
    "            a = self.get_action(s, deterministic)\n",
    "            (s, r, done, _) = env.step(a)\n",
    "            traj['s'].append(s)\n",
    "            traj['a'].append(a)\n",
    "            traj['r'].append(r)\n",
    "            if render: env.render()\n",
    "            if done: break\n",
    "        return {'s': np.array(traj['s']), 'a': np.array(traj['a']), 'r': np.array(traj['r'])}\n",
    "    \n",
    "    def get_action(self, s, deterministic=True):\n",
    "        \"\"\"\n",
    "        Evaluate the agent policy to choose an action, a, given state, s.\n",
    "        \"\"\"\n",
    "        # compute action probabilities\n",
    "        prob_a = self.f_eval(s.reshape(1,-1))\n",
    "        if deterministic:\n",
    "            # choose action with highest probability\n",
    "            return prob_a.argmax()\n",
    "        else:\n",
    "            # sample action from distribution\n",
    "            return (np.cumsum(np.asarray(prob_a)) > np.random.rand()).argmax()\n",
    "    \n",
    "    def _cumulative_discount(self, r, gamma):\n",
    "        \"\"\"\n",
    "        Compute the cumulative discounted rewards (returns).\n",
    "        \"\"\"\n",
    "        r_out = np.zeros(len(r), 'float64')\n",
    "        r_out[-1] = r[-1]\n",
    "        for i in reversed(xrange(len(r)-1)):\n",
    "            r_out[i] = r[i] + gamma * r_out[i+1]\n",
    "        return r_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-01 18:54:36,799] Making new env: Acrobot-v1\n"
     ]
    },
    {
     "ename": "TclError",
     "evalue": "invalid command name \".139858028161648\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTclError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-55ca90a57928>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mWorld\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLearner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m# init environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWorld\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/simonx/Documents/02456-deep-learning/week6/Learner.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mWorld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_cell_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/simonx/Documents/02456-deep-learning/week6/World.py\u001b[0m in \u001b[0;36mset_cell_score\u001b[0;34m(state, action, val)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mgreen\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"#\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mred\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgreen\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"00\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemconfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtriangle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/lib-tk/Tkinter.pyc\u001b[0m in \u001b[0;36mitemconfigure\u001b[0;34m(self, tagOrId, cnf, **kw)\u001b[0m\n\u001b[1;32m   2405\u001b[0m         \u001b[0mthe\u001b[0m \u001b[0mallowed\u001b[0m \u001b[0mkeyword\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mcall\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0mwithout\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2406\u001b[0m         \"\"\"\n\u001b[0;32m-> 2407\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_configure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'itemconfigure'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagOrId\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2408\u001b[0m     \u001b[0mitemconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitemconfigure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2409\u001b[0m     \u001b[0;31m# lower, tkraise/lift hide Misc.lower, Misc.tkraise/lift,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/lib-tk/Tkinter.pyc\u001b[0m in \u001b[0;36m_configure\u001b[0;34m(self, cmd, cnf, kw)\u001b[0m\n\u001b[1;32m   1318\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mStringType\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getconfigure1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcnf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m     \u001b[0;31m# These used to be defined in Widget:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTclError\u001b[0m: invalid command name \".139858028161648\""
     ]
    }
   ],
   "source": [
    "import World, Learner\n",
    "# init environment\n",
    "env = World\n",
    "World\n",
    "actions=Learner.actions\n",
    "\n",
    "# init agent\n",
    "agent = Agent(n_inputs=env.observation_space.shape[0],\n",
    "              n_outputs=actions)\n",
    "# train agent on the environment\n",
    "agent.learn(env, n_epochs=10, learning_rate=0.05, discount_factor=1,\n",
    "            t_per_batch=10000, traj_t_limit=env.spec.timestep_limit, n_early_stop=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# init environment\n",
    "env = gym.make('CartPole-v0')\n",
    "# init agent\n",
    "agent = Agent(n_inputs=env.observation_space.shape[0],\n",
    "              n_outputs=env.action_space.n)\n",
    "# train agent on the environment\n",
    "agent.learn(env, n_epochs=100, learning_rate=0.05, discount_factor=1,\n",
    "            t_per_batch=10000, traj_t_limit=env.spec.timestep_limit, n_early_stop=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# plot training and validation mean reward\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.xlabel('epochs'); plt.ylabel('mean reward')\n",
    "plt.plot(agent.mean_train_rs, label='training')\n",
    "plt.plot(agent.mean_val_rs, label='validation')\n",
    "plt.xlim((0,len(agent.mean_val_rs)-1))\n",
    "plt.legend(loc=2); plt.grid()\n",
    "_=plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# review solution\n",
    "agent.get_trajectory(env, t_limit=1000, render=True)\n",
    "env.render(close=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
