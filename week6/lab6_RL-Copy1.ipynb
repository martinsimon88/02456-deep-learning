{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR (theano.sandbox.cuda): ERROR: Not using GPU. Initialisation of device gpu failed:\n",
      "initCnmem: cnmemInit call failed! Reason=CNMEM_STATUS_OUT_OF_MEMORY. numdev=1\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Cuda error: kernel_reduce_ccontig_node_97496c4d3cf9a06dc4082cc141f918d2_0: out of memory. (grid: 1 x 1; block: 256 x 1 x 1)\n\nApply node that caused the error: GpuCAReduce{add}{1}(<CudaNdarrayType(float32, vector)>)\nToposort index: 0\nInputs types: [CudaNdarrayType(float32, vector)]\nInputs shapes: [(10000,)]\nInputs strides: [(1,)]\nInputs values: ['not shown']\nOutputs clients: [[HostFromGpu(GpuCAReduce{add}{1}.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5ebe07347176>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/simonx/.local/lib/python2.7/site-packages/theano/__init__.pyc\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_initial_driver_test\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msandbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_driver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_nvidia_driver1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m if (config.device.startswith('cuda') or\n",
      "\u001b[0;32m/home/simonx/.local/lib/python2.7/site-packages/theano/sandbox/cuda/tests/test_driver.pyc\u001b[0m in \u001b[0;36mtest_nvidia_driver1\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m             'but got:']+[str(app) for app in topo])\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         raise Exception(\"The nvidia driver version installed with this OS \"\n\u001b[1;32m     40\u001b[0m                         \u001b[0;34m\"does not give good results for reduction.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/simonx/.local/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    869\u001b[0m                     \u001b[0mnode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m                     \u001b[0mthunk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthunk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m                     storage_map=getattr(self.fn, 'storage_map', None))\n\u001b[0m\u001b[1;32m    872\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m                 \u001b[0;31m# old-style linkers raise their own exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/simonx/.local/lib/python2.7/site-packages/theano/gof/link.pyc\u001b[0m in \u001b[0;36mraise_with_op\u001b[0;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;31m# extra long error message in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m     \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/simonx/.local/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Cuda error: kernel_reduce_ccontig_node_97496c4d3cf9a06dc4082cc141f918d2_0: out of memory. (grid: 1 x 1; block: 256 x 1 x 1)\n\nApply node that caused the error: GpuCAReduce{add}{1}(<CudaNdarrayType(float32, vector)>)\nToposort index: 0\nInputs types: [CudaNdarrayType(float32, vector)]\nInputs shapes: [(10000,)]\nInputs strides: [(1,)]\nInputs values: ['not shown']\nOutputs clients: [[HostFromGpu(GpuCAReduce{add}{1}.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "from lasagne.layers import InputLayer, DenseLayer\n",
    "from lasagne.nonlinearities import tanh, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    \"\"\"\n",
    "    Reinforcement Learning Agent\n",
    "    \n",
    "    This agent can learn to solve reinforcement learning tasks from\n",
    "    OpenAI Gym by applying the policy gradient method.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        # symbolic variables for state, action, and advantage\n",
    "        sym_state = T.fmatrix()\n",
    "        sym_action = T.ivector()\n",
    "        sym_advantage = T.fvector()\n",
    "        # policy network\n",
    "        l_in = InputLayer(shape=(None, n_inputs))\n",
    "        l_hid = DenseLayer(incoming=l_in, num_units=20, nonlinearity=tanh, name='hiddenlayer')\n",
    "        l_out = DenseLayer(incoming=l_hid, num_units=n_outputs, nonlinearity=softmax, name='outputlayer')\n",
    "        # get network output\n",
    "        eval_out = lasagne.layers.get_output(l_out, {l_in: sym_state}, deterministic=True)\n",
    "        # get trainable parameters in the network.\n",
    "        params = lasagne.layers.get_all_params(l_out, trainable=True)\n",
    "        # get total number of timesteps\n",
    "        t_total = sym_state.shape[0]\n",
    "        # loss function that we'll differentiate to get the policy gradient\n",
    "        loss = -T.log(eval_out[T.arange(t_total), sym_action]).dot(sym_advantage) / t_total\n",
    "        # learning_rate\n",
    "        learning_rate = T.fscalar()\n",
    "        # get gradients\n",
    "        grads = T.grad(loss, params)\n",
    "        # update function\n",
    "        updates = lasagne.updates.sgd(grads, params, learning_rate=learning_rate)\n",
    "        # declare training and evaluation functions\n",
    "        self.f_train = theano.function([sym_state, sym_action, sym_advantage, learning_rate], loss, updates=updates, allow_input_downcast=True)\n",
    "        self.f_eval = theano.function([sym_state], eval_out, allow_input_downcast=True)\n",
    "    \n",
    "    def learn(self, env, n_epochs=100, t_per_batch=10000, traj_t_limit=None,\n",
    "              learning_rate=0.1, discount_factor=1.0, n_early_stop=0):\n",
    "        \"\"\"\n",
    "        Learn the given environment by the policy gradient method.\n",
    "        \"\"\"\n",
    "        self.mean_train_rs = []\n",
    "        self.mean_val_rs = []\n",
    "        self.loss = []\n",
    "        for epoch in xrange(n_epochs):\n",
    "            # 1. collect trajectories until we have at least t_per_batch total timesteps\n",
    "            trajs = []; t_total = 0\n",
    "            while t_total < t_per_batch:\n",
    "                traj = self.get_trajectory(env, traj_t_limit, deterministic=False)\n",
    "                trajs.append(traj)\n",
    "                t_total += len(traj[\"r\"])\n",
    "            all_s = np.concatenate([traj[\"s\"] for traj in trajs])\n",
    "            # 2. compute cumulative discounted rewards (returns)\n",
    "            rets = [self._cumulative_discount(traj[\"r\"], discount_factor) for traj in trajs]\n",
    "            maxlen = max(len(ret) for ret in rets)\n",
    "            padded_rets = [np.concatenate([ret, np.zeros(maxlen-len(ret))]) for ret in rets]\n",
    "            # 3. compute time-dependent baseline\n",
    "            baseline = np.mean(padded_rets, axis=0)\n",
    "            # 4. compute advantages\n",
    "            advs = [ret - baseline[:len(ret)] for ret in rets]\n",
    "            all_a = np.concatenate([traj[\"a\"] for traj in trajs])\n",
    "            all_adv = np.concatenate(advs)\n",
    "            # 5. do policy gradient update step\n",
    "            loss = self.f_train(all_s, all_a, all_adv, learning_rate)\n",
    "            train_rs = np.array([traj[\"r\"].sum() for traj in trajs]) # trajectory total rewards\n",
    "            eplens = np.array([len(traj[\"r\"]) for traj in trajs]) # trajectory lengths\n",
    "            # compute validation reward\n",
    "            val_rs = np.array([self.get_trajectory(env, traj_t_limit, deterministic=True)['r'].sum() for _ in range(10)])\n",
    "            # update stats\n",
    "            self.mean_train_rs.append(train_rs.mean())\n",
    "            self.mean_val_rs.append(val_rs.mean())\n",
    "            self.loss.append(loss)\n",
    "            # print stats\n",
    "            print '%3d mean_train_r: %6.2f mean_val_r: %6.2f loss: %f' % (epoch+1, train_rs.mean(), val_rs.mean(), loss)\n",
    "            # render solution\n",
    "            #self.get_trajectory(env, traj_t_limit, render=True)\n",
    "            # check for early stopping: true if the validation reward has not changed in n_early_stop epochs\n",
    "            if n_early_stop and len(self.mean_val_rs) >= n_early_stop and \\\n",
    "                all([x == self.mean_val_rs[-1] for x in self.mean_val_rs[-n_early_stop:-1]]):\n",
    "                break\n",
    "    \n",
    "    def get_trajectory(self, env, t_limit=None, render=False, deterministic=True):\n",
    "        \"\"\"\n",
    "        Compute trajectroy by iteratively evaluating the agent policy on the environment.\n",
    "        \"\"\"\n",
    "        t_limit = t_limit or env.spec.timestep_limit\n",
    "        s = env.reset()\n",
    "        traj = {'s': [], 'a': [], 'r': [],}\n",
    "        for _ in xrange(t_limit):\n",
    "            a = self.get_action(s, deterministic)\n",
    "            (s, r, done, _) = env.step(a)\n",
    "            traj['s'].append(s)\n",
    "            traj['a'].append(a)\n",
    "            traj['r'].append(r)\n",
    "            if render: env.render()\n",
    "            if done: break\n",
    "        return {'s': np.array(traj['s']), 'a': np.array(traj['a']), 'r': np.array(traj['r'])}\n",
    "    \n",
    "    def get_action(self, s, deterministic=True):\n",
    "        \"\"\"\n",
    "        Evaluate the agent policy to choose an action, a, given state, s.\n",
    "        \"\"\"\n",
    "        # compute action probabilities\n",
    "        prob_a = self.f_eval(s.reshape(1,-1))\n",
    "        if deterministic:\n",
    "            # choose action with highest probability\n",
    "            return prob_a.argmax()\n",
    "        else:\n",
    "            # sample action from distribution\n",
    "            return (np.cumsum(np.asarray(prob_a)) > np.random.rand()).argmax()\n",
    "    \n",
    "    def _cumulative_discount(self, r, gamma):\n",
    "        \"\"\"\n",
    "        Compute the cumulative discounted rewards (returns).\n",
    "        \"\"\"\n",
    "        r_out = np.zeros(len(r), 'float64')\n",
    "        r_out[-1] = r[-1]\n",
    "        for i in reversed(xrange(len(r)-1)):\n",
    "            r_out[i] = r[i] + gamma * r_out[i+1]\n",
    "        return r_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "TclError",
     "evalue": "invalid command name \".139748286604280\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTclError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8d31fa786cf2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mWorld\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLearner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# init environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWorld\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mWorld\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/simonx/Documents/02456-deep-learning/week6/Learner.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mWorld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_cell_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/simonx/Documents/02456-deep-learning/week6/World.pyc\u001b[0m in \u001b[0;36mset_cell_score\u001b[0;34m(state, action, val)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mgreen\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"#\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mred\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgreen\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"00\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemconfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtriangle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/lib-tk/Tkinter.pyc\u001b[0m in \u001b[0;36mitemconfigure\u001b[0;34m(self, tagOrId, cnf, **kw)\u001b[0m\n\u001b[1;32m   2405\u001b[0m         \u001b[0mthe\u001b[0m \u001b[0mallowed\u001b[0m \u001b[0mkeyword\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mcall\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0mwithout\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2406\u001b[0m         \"\"\"\n\u001b[0;32m-> 2407\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_configure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'itemconfigure'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagOrId\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2408\u001b[0m     \u001b[0mitemconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitemconfigure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2409\u001b[0m     \u001b[0;31m# lower, tkraise/lift hide Misc.lower, Misc.tkraise/lift,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/lib-tk/Tkinter.pyc\u001b[0m in \u001b[0;36m_configure\u001b[0;34m(self, cmd, cnf, kw)\u001b[0m\n\u001b[1;32m   1318\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mStringType\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getconfigure1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcnf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m     \u001b[0;31m# These used to be defined in Widget:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTclError\u001b[0m: invalid command name \".139748286604280\""
     ]
    }
   ],
   "source": [
    "import World, Learner\n",
    "# init environment\n",
    "env = World\n",
    "World\n",
    "actions=Learner.actions\n",
    "\n",
    "# init agent\n",
    "agent = Agent(n_inputs=env.observation_space.shape[0],\n",
    "              n_outputs=actions)\n",
    "# train agent on the environment\n",
    "agent.learn(env, n_epochs=10, learning_rate=0.05, discount_factor=1,\n",
    "            t_per_batch=10000, traj_t_limit=env.spec.timestep_limit, n_early_stop=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-01 20:14:36,326] Making new env: CartPole-v0\n",
      "[2017-03-01 20:14:36,647] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 mean_train_r:  21.91 mean_val_r:  24.20 loss: 1.800766\n",
      "  2 mean_train_r:  23.44 mean_val_r:  24.90 loss: 2.225367\n",
      "  3 mean_train_r:  22.36 mean_val_r:  16.10 loss: 1.763844\n",
      "  4 mean_train_r:  22.85 mean_val_r:  20.80 loss: 2.282690\n",
      "  5 mean_train_r:  22.95 mean_val_r:  18.30 loss: 2.197094\n",
      "  6 mean_train_r:  22.90 mean_val_r:  18.90 loss: 2.282936\n",
      "  7 mean_train_r:  22.09 mean_val_r:  21.60 loss: 2.263440\n",
      "  8 mean_train_r:  23.45 mean_val_r:  22.60 loss: 2.485180\n",
      "  9 mean_train_r:  21.99 mean_val_r:  25.30 loss: 2.343019\n",
      " 10 mean_train_r:  22.12 mean_val_r:  23.10 loss: 1.984752\n"
     ]
    }
   ],
   "source": [
    "# init environment\n",
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "# init agent\n",
    "agent = Agent(n_inputs=env.observation_space.shape[0],\n",
    "              n_outputs=env.action_space.n)\n",
    "# train agent on the environment\n",
    "agent.learn(env, n_epochs=10, learning_rate=0.05, discount_factor=1,\n",
    "            t_per_batch=10000, traj_t_limit=env.spec.timestep_limit, n_early_stop=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# plot training and validation mean reward\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.xlabel('epochs'); plt.ylabel('mean reward')\n",
    "plt.plot(agent.mean_train_rs, label='training')\n",
    "plt.plot(agent.mean_val_rs, label='validation')\n",
    "plt.xlim((0,len(agent.mean_val_rs)-1))\n",
    "plt.legend(loc=2); plt.grid()\n",
    "_=plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# review solution\n",
    "agent.get_trajectory(env, t_limit=1000, render=True)\n",
    "env.render(close=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
